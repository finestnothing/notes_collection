up:: [[+info_databricks_training_notes]]

# 202211081511

Created: 2022-11-08 15:11

# DE 4.1 - DLT UI Walkthrough

## Creating and Configuring a Pipeline

workflows > delta live tables > create pipeline

1. Product Edition: select "Advanced"
2. Pipeline Name: Whatever name you want
3. Notebook Libraries: path to notebook that has DLT[^1]s in it
4. Configuration > Add Configuration
	1. key: source
	2. value: {data stream source}
5. Storage Location: {location to store logs, file, etc; optional}
6. Target: {something here I guess}
7. Pipeline mode:
	1. Triggered - Run once then stop until triggered again
	2. Continuous - Ingests and processes data as it arrives
8. Cluster mode:
	1. Legacy autoscaling - Ideal option by default. Scales workers up or down based on workloads
	2. Enhanced Autoscaling - Scales workers up or down based on workloads
	3. Fixed Size - Sets a consistent number of workers
9. Specify the min and max workers, will automatically change the DBU per hour amount
10. Photon Acceleration - Can be good for parallelization
11. Channel - Not sure, wasn't covered
12. Policy - Not sure, wasn't covered
13. Click Create
14. Modes
	1. Development
		1. Reuses the same cluster instead of making a new one each time
			1. First run is slow, later runs are usually faster
		2. Disables retries
	2. Production
		1. Keeps retries
		2. new cluster for each run to keep them separate
	3. Docs have more info about these options

# DE 4.2 - Python Vs SQL

Differences between Python and SQL Delta Live Tables

| python                                                                          | sql                                                       | notes                                                                                                                                                                                  |
| ------------------------------------------------------------------------------- | --------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| No DLT Module, no syntax checking                                               | Has syntax checking                                       | Running a python DLT cell will show an error. SQL will check for syntactic validity                                                                                                    |
| DLT module should be explicitly imported                                        |                                                           |                                                                                                                                                                                        |
| Tables as dataframes                                                            | Tables as query results                                   | In python, you can string multiple API calls together. In SQL, all sub-transformations have to be saved in temporary tables first                                                      |
| @dlt.table()                                                                    | Select Statements                                         | In SQL, the logic for the tables is contained in the SQL query of the table definition. In Python, data transformations are specified when you configure each option for @dlt.tables() |
| @dlt.table(comment = "Python Comment", table_properties = {"quality":"silver"}) | COMMENT "SQL Comment" TBLPROPERTIES("quality" = "silver") | Adding comments and table properties                                                                                                                                                   |

# DE 4.3 - Pipeline Results

As long as a target database is specified during pipeline creation, they will go to a specific database and be accessible as tables there

# DE 4.4 - Pipeline Event Logs

Stores much of the important info used to manage and report pipelines.

```python
event_log_path = DA.paths.storage_location/system/events
event_log = spark.read.format("delta").load(event_log_path)
event_log.createOrReplaceTempView("event_log_raw")

display(event_log)
```

Shows log data about what happened during the pipeline execution

```
latest_update_id = spark.sql("""
	select origin.update_id
	from event_log_raw
	where event_type = "create_update"
	order by timestamp desc limit 1
""").first().update_id

print("Last Update ID: {latest_update_id}")

spark.conf.set("latest_update.id", latest_update_id)
```

Retrieve the most recent update ID with spark

## Audit Logging

```sql
select timestamp, detail:user_action:action, details:user_action:user_name
from event_log_raw
where event_type = "user_action"
```

Gets events relating to the pipeline

### Lineage

```sql
select details:flow_definition.output_dataset, details:flow_defiition.input_datasets
from event_log_raw
where event_type = "flow_definition"
	and origin.update_id = '${latest_update_id}'
```

Shows lineage for how data flows through the tables

## Data Quality Metrics

```sql
select row_expectations.dataset as dataset,
	row_expecations.name as expectation,
	sum(row_expectations.passed_records) as passing_records,
	sum(row_expectations.failed_records) as failing_records
from (
	select explode(
		from_json(details :flow_progress :data_quality : expectations,
			"array<struct<name: string, dataset: string, passed_records: int, failed_records: int>>")
	) row_expectations
	from event_log_raw
	where event_type = "flow_progress"
		and origin.update_id = '${latest_update.id}'
)
group by row_expectations.dataset, row_expectations.name
```

Captures metrics for each constraint in the entire lifetime of the table

[^1]: Delta Live Tables
