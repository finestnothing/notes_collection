up:: [[+info_databricks_training_notes]]

# 202211071456

Created: 2022-11-07 14:56

# DE 3.3 Setting Up Delta Tables

## Early Tables

Early stage tables should have the raw data
No validation or type checking should occur
This ensures that if there are any issues with the checking later down the line, no data is actually deleted.

## CTAS[^1]

```sql
create or replace table sales as 
select * from parquet.`${DA.paths.dataset}/ecommerce/raw/sales-historical`;
```

Create table how it normally is done
This does not have additional options for reading from files

```sql
create or replace temp view sales_tmp_view
	({column} {type}, ...)
using csv
options (
	path = {path},
	header = {"true"|"false"},
	delimiter = {"|", ",", ...}
);

create tables sales_data as
	select * from sales_tmp_view;

select * from sales_data
```

This lets us specify csv file options to correctly parse a csv file into a table rather than hoping that it is delimited in the default manner

## Filtering and Renaming Columns From Existing Tables

```sql
create or replace purchases as 
select order_id as id, transaction_timestamp, purchase_revenue_in_usd as price
from sales

select * from purchases
```

Creates a table with changed field names

## Generated Columns

```sql
create or replace table purchase_dates (
	id STRING,
	transaction_timestamp TIMESTAMP,
	price STRING,
	date DATE GENERATED ALWAYS AS (
		cast(cast(transaction_timestamp/1e6 as TIMESTAMP) AS DATE)
		COMMENT "generated based on `transaction_timestamp`"
	)
)
```

## Merge Statment

```sql
set spark.databricks.delta.schema.automerge.enabled=true

merge into purchase_dates a
using purchases b
on a.id = b.id
when not matched then
	insert *
```

This allows for generating columns when using a delta lake `merge` statement.[^2]
It will be covered more later.

## Table Constraints

```sql
alter table purchase_dates add constraint valid_date check (date > '2020-01-01');
```

Adds a table constraint to the table to make sure the data is still good
Two kinds of restraints: `NOT NULL` and `CHECK`
When doing `describe extend {table}`, the contraints will be in the `tblproperties` field

## Additional Table Options and Metadata

```sql
create or replace table users_pii
comment "contains PII"
Location "${location}"
partitioned by (first_touch_date)
as 
	select
		first_touch_date,
		current_timestamp updated,
		input_tile_name() source_file
	from parquet.{path}
```

These are some options for adding more info to tables
As a note, you should avoid partitioning data in most cases
A new directory is made for each partition data value to store relevant info, which can slow it down if there are a significant number of unique values

## Cloning Delta Lake Tables

There are two options for cloning, `deep clone` and `shallow clone`
`deep clone` fully copies data from a source table to a target
	This occurs incrementally so executing the command again will sync any changes that have happened in the source
	Includes metadata and data
	Essentially passing by value
`shallow clone` only copies delta transaction logs so the data doesn't move
	Essentially passing by reference

```sql
create or replace table purchase_deep_clone
deep clone purchases

create or replace table purchase_shallow_clone
shallow clone purchases
```

# DE 3.4 Load Data into Delta Lake

## Overwriting Tables

Overwriting is much better than replacing tables
- Pros
	- Faster
	- Old version still exists so it can be accessed using timetravel tools
	- Atomic operation so queries can still hit the table while you are deleting the table
	- Due to ACID transaction guarantees, if the overwrite fails the table will remain in its previous state like nothing happened

```sql
create or replace table events as 
select * from parquet.{stuff}
```

This is the standard way to restart a table
It replaces the table as it executes, so all history and metadata disappears and it cannot be queried during it

```sql
insert overwrite sales
select * from parquet.{path}
```

Only works on existing tables, cannot create a new table
Has all the benefits of overwriting tables
An only overwrite with a matching schema so it is safer than possibly making a new schema with bad data

## Append Data

```sql
insert into sales
select * from parquet.{path}
```

Does not delete any data, only appends it to the bottom of the table
Allows for incremental updates to tables rather than overwriting it every time there is a change in the data
Note: there are no built in guarantees that the data is not writing the same records multiple times

## Merge Updates

Supports more than pure sql
Merge: inserts, updates, and deletes

```sql
merge into target a
using source b
on {merge_condition}
when matched then {matched_action}
when note matched then {non_matched_option}
```

Basic merge syntax

```sql
create or replace temp view users_update as 
select *, current_timestamp() as updated
from parquet.{path}
```

Setting up basic table

```sql
MERGE INTO users a
USING users_update b
ON a.user_id = b.user_id
WHEN MATCHED
	AND a.email is null 
	AND b.email is not null
	THEN UPDATE SET email = b.email, updated = b.updated
WHEN NOT MATCHED THEN
	INSERT *
```

Merges the data from b into a
If the user_id's match and target.email is null and source.email is not, then target.email is set as source.email
If there isn't a match for the user_id, then all data is overwritten

```sql
MERGE INTO events a
USING events_update b
ON a.user_id = b.user_id
	AND a.event_timestamp = b.event_timestamp
WHEN NOT MATCHED
	AND b.traffic_source = 'email'
	THEN INSERT INTO *
```

This is a way to do logging without duplication. If the user_id and timestamps already match then the event is not needed, if there isn't a match then it adds the records
Basically this is a safe append that eliminates duplication

## Incremental Loading

```sql
COPY INTO sales
FROM {parquet_path}
FILEFORMAT = PARQUET
```

#to/do - Let Sam know about this!
Incrementally ingests data from external systems (wherever the parquet files are)
- Expectations:
	- Schema has to be consistent
	- Duplicate records should be excluded or handled downstream, not here
Potentially much cheaper since it doesn't do a full table scan
This command gets run multiple times, each time copying any other changes and such
	A single run just copies the data which can be done a lot of other ways
	Subsequent runs will only copy over new data and other changes (As new files are picked up in the source folder automatically)
Works best for data that grows predictably

[^1]: Create Table as Select
[^2]: If a field that would otherwise be generated is included in an insert statement, the insert will fail *unless* the value exactly matches what would have been generated.
