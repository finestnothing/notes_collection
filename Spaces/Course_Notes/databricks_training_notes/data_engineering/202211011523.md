up:: [[+info_databricks_data_engineering_training_notes]]
 

# 202211011523

Created: 2022-11-01 15:23

## DE 2.6L - Reshape Data Lab

### Pivoting Table to Get Counts From a Column

```sql
create or replace temp view events_pivot as
select * from (
	select user_id user, event_name
	from events
) pivot (
	count(*) for event_name in (
		"cart", "pillows", "login", "main", "careers", "guest", "faq", "down",
		"warranty", "finalize", ... {all possible values that we want to look for}
	)
)
```

```python
spark.read.table("events")
	.groupby("user_id")
	.pivot("event_name")
	.count()
	.withColumnRenamed("user_id", "user")
	.createOrReplaceTempView("events_pivot")
```

Here we are trying to get the count of each event type in the "event_name" column.
The python version is *much* easier
What happens:
	saved in a view called events_pivot
	pivot the "event_name" column to get a count of each value, then convert that to new columns
	Also get the "user_id" column so each of the counts in the other columns is specific to a user (not total)

```sql
create or replace temp view clickpaths as(
	select *
	from events_pivot a
		join transactions b on a.user = b.user_id
)
```

```python
from pyspark.sql.functions import col
spark.read.table("events_pivot")
	.join(
		spark.table("transactions"), col("events_pivot.user") == col("transactions.user_id"), "inner"
	)
	.createOrReplaceTempView("clickpaths")
```

Here we are joining the events_pivot table (above) with transactions to create clickpaths.

## DE 2.7A - SQL UDFs

UDF - User defined functions
Natively in DB runtime 9.1 and on
How to combine case when clauses
UDF's use spark under the hood so it keeps those optimizations
SQL UDF's exist as objects in the meta store

```sql
create or replace function sale_announcement(
	item_name STRING,
	item_price INT
)
RETURNS STRING
RETURN concat("The ", item_name, " is on sale for $", round(item_price * 0.8, 0));

select
	*,
	sale_announcement(name, price) as message
from
	item_lookup
```

Create a simple UDF that returns a string with variables inserted

### Scoping and Permissions of SQL UDF's

```sql
DESCRIBE FUNCTION EXTENDED sale_announcement
```

This gives info about it
function: name and location
type: scalar
input: item_name STRING
	item_price INT
returns: string
deterministic: true
owner: root
create time: {time}
body: {copy of function code}

```sql
create or replace function item_preference(
	name STRING
	price INT
)
returns string
return
	case
		when name = "Standard Queen Mattress"
			then "This is my default mattress"
		when name = "Premium Queen Mattress"
			then "This is my favorite mattress"
		when price > 100
			then concat("I'd wait until the ", name, " is on sale for $", round(price * 0.8, 0))
	end;

select
	*,
	item_preference(name, price)
from
	item_lookup
```

## Python UDF's

```python
def first_letter_function(email):
	return email[0]

first_letter_function("annagray@gmail.com")
```

basic python defined function, nothing special

```python
first_letter_udf = udf(first_letter_function)
```

Register it as a UDF for use with pyspark
Serializes the function and sends it to executors

```python
display(sales_df.select(first_letter_udf(col("email"))))
```

Runs the UDF in pyspark

```python
first_letter_udf = spark.udf.register("sql_udf", first_letter_function)
```

Registers the first_letter_function under the name "sql_udf" for using in sql, and under the name "first_letter_udf" for use in python (pyspark)

```python
display(sales_df.select(first_letter_udf(col("email"))))
```

```sql
select sql_udf(email) as first_letter from sales
```

Both of these call the first_letter_function for each row of data

### Python UDF's - Decorator Syntax

Lets you define and register a UDF using Python decorator syntax
The @udf parameter is the column datatype the function returns
No longer able to call the local python function (first_letter_function("annagray@gmail.com"))

```python
@udf("string")
def first_letter_udf(email: str) -> str:
	return email[0]
```

```python
from pyspark.sql.functions import col
sales_df = spark.table("sales")
display(sales_df.select(first_letter_udf(col("email"))))
```

Not sure why we would want to use decorator syntax, but it works and is pretty cool I guess

## Pandas/Vectorized UDFs

Pandas UDF's are also available in python
These make UDF's more efficient since pandas uses apache arrow to speed up computation
Apache arrow is an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and python processes with near zero serialization/deserialization cost

Note: For spark 3.0+, always define pandas UDF's with type hints
``

```python
import pandas as pd
from pyspark.sql.functions import pandas_udf

# Decorator Syntax
@pandas_udf("string")
def vectorized_udf(email: pd.series) -> pd.Series:
	return email.str[0]

# or
# Normal syntax
def vectorized_udf(email: pd.Series) -> pd.Series:
	return email.str[0]
vectorized_udf = pandas_udf(vectorized_udf, "string")
```

Define the pandas UDF's

```python
display(sales_df.select(vectorized_udf(col("email"))))
```

Run it in python

```python
spark.udf.register("sql_vectorized_udf", vectorized_udf)
```

Register the pandas UDF to the sql namespace

```sql
select
	sql_vectorized_udf(email) as firstLetter
from
	sales
```

Run the pandas UDF from SQL

## Done for Today - Next Lesson is DE 3.1
