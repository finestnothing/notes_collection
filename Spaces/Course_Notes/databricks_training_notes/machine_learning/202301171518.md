up:: [[+info_databricks_machine_learning_training]]

# 202301171518

Created: 2023-01-17 15:18

## Spark

- RDD
	- Distributed collection of JVM objects
	- Becomes DataFrame
- Dataframe
	- Distributed collection of row objects
	- Uses expressions and UDF's
	- Logical plans and optimizer
	- Became Dataset
	- Still most common though
- Dataset
	- Uses rows internally, jvm externally
	- Slower than dataframe
	- Gets some benefits from Dataframes

## Machine Learning

- What is Machine Learning
	- Learns patterns and relationships in data without explicitly programming them
		- Can derive approximations along with confidences

### ML Types

#### Supervised Learning

- Train with known function output
- Can determine regression or classification problems
- Regression
	- Predict a number (1-10)
- Classification
	- Predict a category
		- E.G. Yes/No, cat/dog/hamset

#### Unsupervised Learning

- Unlabeled data
- No known function output
- Clustering data is here (finding groups in data)
- Dimensionality Reduction (get rid of features that aren't used)

#### Reinforcement Learning

- States, action, and rewards
- If an action results in a success, reward to increase the weight on those
- If a failure, the opposite

### Workflow

1. Define business use case
2. Define success, constraints, and infrastructure
3. Data collation (find ideal data set, real data set, etc)
4. Feature engineering (clean/process)
5. Modeling
6. Deployment
7. Go back to step 1 if needed

- Measure success
	- Determine if False Negative or False Positive is better or worse
		- Rain: Either have umbrella and not need one, or need one and not have one
		- Medical: Could have a treatment that is a false positive, and doesn't actually helps but gets used
- Data visualization is very helpful for testing regression ML models

## Data Cleaning

- Fixing a data type (string -> double)

```python
fixed_price_df = base_df.withColumn("price", translate(col("price"), "$,", "").cast("double"))
```

Removes the $ and converts result to a double from string

- Keep only rows with positive value in a column

```python
pos_price_df = fixed_price_df.filter(col("price") > 0)
```

- Get and show distinct values (of minimum_nights)

```python
	display(
		pos_price_df
		.groupBy("minimum_nights").count()
		.orderBy(col("count")).desc(), col("minimum_nights"))
)
```

### Null Values

- How to handle
	- Drop any record with any nulls
	- Replace nulls with mean/median/zero/mode/etc
	- Use Alternating Least Squares (ALS) to impute missing values
		- If using this, must make a new column/field specifying that something was imputed

#### Imputer

- SparkML requires all fields to be of type double to impute/

```python
from pyspark.sql.functions import col
from pyspark.sql.types import IntegerType

integer_columns = [x.name for x in min_nights_df.schema.fields if x.dataType == IntegerType()]
doubles_df = min_nights_df

for c in integer_columns:
    doubles_df = doubles_df.withColumn(c, col(c).cast("double"))

columns = "\n - ".join(integer_columns)
print(f"Columns converted from Integer to Double:\n - {columns}")
```

Automatically converts all "integer" type fields/columns to double, instead of specifying them all manually

- Create a new column to denote if imputer was used or not for all relevant columns

```python
from pyspark.sql.functions import when

impute_cols = [
    "bedrooms",
    "bathrooms",
    "beds", 
    "review_scores_rating",
    "review_scores_accuracy",
    "review_scores_cleanliness",
    "review_scores_checkin",
    "review_scores_communication",
    "review_scores_location",
    "review_scores_value"
]

for c in impute_cols:
    doubles_df = doubles_df.withColumn(c + "_na", when(col(c).isNull(), 1.0).otherwise(0.0))
```

##### Transformers and Estimators

- Transformer transforms one dataframe onto another
	- Does not learn from data
	- Has a .transform() method
- Estimator
	- Learns parameters from the data
	- Tries to predict a value
	- has a .fit() method
- These work together as an imputer to fill in null values

```python
from pyspark.ml.feature import Imputer

imputer = Imputer(strategy="median", inputCols=impute_cols, outputCols=impute_cols)

imputer_model = imputer.fit(doubles_df)
imputed_df = imputer_model.transform(doubles_df)
```

Try to fill in null values with the median value using an estimator
	Then, transform the estimated values using the actual data (transform it to match)

```python
imputed_df.write.format("delta").mode("overwrite").save(f"{DA.paths.working_dir}/imputed_results")
```

Save the dataframe with median imputed values to a file
