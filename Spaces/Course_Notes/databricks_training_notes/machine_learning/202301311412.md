up:: [[+info_databricks_machine_learning_training]]
source::

# 202301311412

Created: 2023-01-31 14:12

# Linear Regression with Spark

- Residual - Amount that the model is off
	- Positive residual
		- Real value is lower than predicted value
	- Negative residual
		- Real value is higher than predicted value
	- The goal of finding a line of best fit is to find a line that minimizes the sum of the squared residual values
- Loss
	- Positive and negative errors cancel out
	- y-y0
- Absolute loss
	- Absolute value of residuals'
	- |y-y0|
- Squared loss
	- Can be differentiated
	- (y-y0)^2
	- Used most often
- RMSE
	- Root Mean Squared Error
	- sqrt(1/n * sum(i = 1 to n, (y - yi)^2))
	- Want close to 0
- Assumptions needed for Linear Regression
	- Linear relationship between x and y
		- Not cubic, 4th power, etc
	- Observations are independent
	- Y is normally distributed
	- Variance of residual is the same for any point
- R squared
	- 1 - Perfect Prediction for every value
	- 0 - Always Predicts Average
	- infinity - Significantly worse than predicting average
	- Want close to 1

Scikit-learn is great for single-node machine learning
Pyspark is great for larger models/training
Spark libraries:
	ML Lib (old)
	Spark ML (new)

# Machine Learning Demo in Spark

```python
# This randomly splits the data_df into two dataframes, with train_df having 80% of the data
train_df, test_df = data_df.randomSplit([0.8, 0.2], seed = 42)
```

```python
# Randomly splits data, with partitions.
train_df, test_df = (data_df
					 .repartition(24)
					 .randomsplit([0.8, 0.2], seed = 42)
					 )
```

```python
# Transforms the data with VectorAssembler
# Creates Linear Regression model
# Trains the Linear Regression model with .fit()
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorAssembler

vec_assembler = VectorAssembler(inputCols = ["bedrooms"], outputCol = "features")

vec_train_df = vec_assembler.transform(train_df)

lr = LinearRegression(featuresCol = "bedrooms", labelCol = "price")
lr_model = lr.fit(vec_train_df)
```

```python
m = lr_model.coefficients[0] # Get Linear Regression coefficient
b = lr_model.intercept # Get the Y intercept
```

```python
# Apply the linear regression model to the test dataframe
vec_test_df = vec_assembler.transform(test_df)

pred_df = lr_model.transform(vec_test_df)

pred_df.select("bedrooms", "features", "price", "prediction").show()
```

```python
# Gets RMSE score of model
from pyspark.ml.evaluation import RegressionEvaluator

regression_evaluator = RegressionEvaluator(predictionCol = "prediction", labelCol = "price", metricName = "rmse")

rmse = regression_evaluator(pref_df)
```

# Non-Numeric Features

- Categorical
	- Single-feature categories
	- Dog, Cat, Fish
- Ordinal
	- Categories of a single feature
	- Infant, Toddler, Adolescent, Teen, Young Adult, etc

## How to Apply Linear Regression to Non-numeric Features

- You can't by just picking a number like cat = 1, dog = 2
- Use one-hot encoding (OHE) in binary
	- columns = dog | cat | fish
	- dog = 100 (yes dog, no cat, no fish)
	- cat = 010 (no dog, yes cat, no fish)
	- fish = 001 (no dog, no cat, yes fish)
- OHE isn't good at scale, instead use sparse vectors
	- DenseVector = (0, 0, 0, 7, 0, 2, 0, 0, 0, 0)
	- SparseVector = (10, [3, 5], [7, 2])
		- `(number of elements, [indices of non-zero elements], [values of non-zero elements])`

```python
from spark.ml.feature import OneHotEncoder, StringIndexer

categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == "string"]
index_output_cols = [x + "Index" for x in categorical_cols]
ohe_output_cols = [x + "OHE" for x in categorical_cols]

string_indexer = StringIndexer(inputCols = categorical_cols, outputCols = index_output_cols, handleInvalid = "skip")
ohe_encoder = OneHotEncoder(inputCols = index_output_cols, outputCols = ohe_output_cols)
```

```python
from pyspark.ml.feature import VectorAssembler

number_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == "double") & (field != "price"))]
assembler_inputs = ohe_output_cols + numeric_cols
vec_assembler = VectorAssembler(inputCols = assembler_inputs, outputCol = "features")
```

```python
from pyspark.ml.regression import LinearRegression

lr = LinearRegression(labelCol = "price", featuresCol = "features")
```

```python
# Creates a pipeline of all the steps from the raw data through training the linear regression model
# Applies it to the train_df, but also means we can easily run our test_df through it rather than redoing everything
from pyspark.ml import pipeline

stages = [string_indexer, ohe_encoder, vec_assembler, lr]
pipeline = Pipeline(stages=stages)

pipeline_model = pipeline.fit(train_df)
```

# Data Cleansing

```python
# Replace all $ and , in the "price" column with an empty string
# Cast as double
data_df.withColumn("price", translate(col("price"), "$,", "").cast("double"))
```

```python
# Generates summary data
display(data_df.describe())

# Generates additional summary data, including quartiles for numeric cols
display(data_df.summary())
```

```python
from pyspark.sql.functions import col
from pyspark.sql.types import IntegerType

integer_columns = [x.name for x in min_nights_df.schema.fields if x.dataType == IntegerType()]
doubles_df = min_nights_df

for c in integer_columns:
	doubles_df = doubles_df.withColumn(c, col(c).cast("double")) # Casts all integer columns as doubles in doubles_df

columns = "\n - ".join(integer_columns) # Shows what columns were converted to doubles
```

```python
# For all columns in impute_cols of doubles_df
# Create a new column to keep track of if the value was null or not
# 1 means it was imputed later, 0 means it was not
from pyspark.sql.functions import when

impute_cols = [
	"bedrooms",
	"bathrooms",
	...
]

for c in impute_cols:
	doubles_df = doubles_df.withColumn(c + "_na", when(col(c).isNull(), 1.0).otherwise(0.0))
```

An Imputer estimates the value to insert in place of a null value.
This can be mean, median, mode, average, etc

```python
# Replaces all null values with the average value of the non-null values
from pyspark.ml.feature import Imputer

imputer = Imputer(inputCols=["a", "b"], outputCols=["out_a", "out_b"])
model = imputer.fit(data_df)

model.transform(data_df).show()
```

```python
# Other options for how to fill in the null values
imputer = Imputer(strategy = {"median"|"mode"|"mean"})
```

```python
# Used to see if the column "price" follows a log normal distribution
# Display this data and see if it makes a rough bell curve
data_df.select(log("price"))
```

# MLflow Tracking

- ML Flow is open source
	- Helps to manage machine learning
		- tracks experiments
		- makes code reproduceable
		- packages and deploys models

```python
import mlflow
import mlflow.spark
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import RegressionEvaluator

with mlflow.start_run(run_name = "LR-Single-Feature") as run:
	# Define pipeline
	# Single feature (bedrooms) linear regression model to predict price
	vec_assembler = VectorAssembler(inputCols = ["bedrooms"], outputCols = "features")
	lr = LinearRegression(featureCols = "features", labelCol = "price")
	pipeline = Pipeline(stages = [vec_assembler, lr])
	pipeline_model = pipeline.fit(train_df)

	# Logging Parameters
	mlflow.log_param("label", "price") # Logs what label was used
	mlflow.log_param("features", "bedrooms") # Logs what feature(s) were used

	# Logging Model
	mlflow.spark.log_model(pipeline_model, "model", input_example = train_df.limit(5).toPandas()) # Logs data regarding the model

	# Evaluate Predictions
	pred_df = pipeline_model.transform(test_df)
	regression_evaluator = RegressionEvaluator(
		predictionCol = "prediction", 
		labelCool = "price", 
		metricName = "rmse"
	)
	rmse = regression_evaluator.evaluate(pred_df)

	# Log Metrics
	mlflow.log_metric("rmse", rmse)
```
