source:: Data Engineering with Databricks - Module 1

# 202210111615
Created: 2022-10-11 16:15

## Databricks machine learning
- Objectives
	- Overview of it
	- Identify how to use it
	- Components and functionalities
	- Successful use cases
- Databricks is data native
	- Data is in the same place, so no need to move it around
- Can be an end to end system for ML lifecycle
	- Prep data
	- Build Model
	- Deploy and monitor model
	- Uses delta lake and spache spark
	- Batch scoring
	- Cloud training
- Common problems
	- Quality issues
		- Data built in, won't get corrupted or lost trying to move it around
	- Compute resources - too much or not enough resources for training
		- Auto scaling for resources
		- Can also manually specify resources
	- Need input data sets for training and testing
		- Feature score registry to save the data
	- Develop models - Can be costly and time consuming to develop
		- Databricks AutoML builds a series of models and logs their runtimes and effectiveness
	- Hard to evaluate and govern new ML settings
		- Access control lists and Unity Catalog work together to restrict access
	- Once they're built, need to be integrated
		- Store and manage models
		- Standard model deployment and staging 
		- Streaming serving tools
- Components in Databricks
	- Databricks ML Runtime
		- Create environments for running modles
		- Has tensorflow, scikit learn, keras, etc
	- Workflows
		- Automate running jobs
	- Repos
		- Sync projects and existing projects
		- CI/CD workflows
- MLflow Basics
	- Open source tool
	- Manage ML lifecycle
	- Tracks model runs within experiments
		- Runs - Single run of the code, actual use
		- Experiments - Training
	- Models - manage and deploy models
	- Projects - package and share ML code
	- Model Registry - Centralize model store to manage ML lifecycle
	- Model serving - Host ML models for real time servings
- AutoML
	- Automatically apply ML to dataset
	- Uses open source ML models on a dataset automatically
		- Glassbox though, so you can always see what code is being run
		- Can add and edit the code that it finds
	- Can be used through interface or in python
	- Ideal for exploration phase of picking the model type
- Feature Store
	- Stores features (aka input variables) for training
	- Makes features available across the organization for reuse
	- Co-designed with ML portion of databricks
	- Feature store inherits Delta lake benefits
	- When a model is trained in feature store, store what features it was trained with in MLFlow

What the heck is Koalas? #to/research 
Got my [[fundamentals_of_the_databricks_lakehouse_platform_accreditation.pdf]]