source:: Databricks Data Engineering with Databricks V3

# 202210271455
Created: 2022-10-27 14:55

Switching to V3 of the course
- Reset progress, but will be the information to take to prepare for the exam update that rolls out on [[20221119]]
Changes in V3
- Unity catalog optional module added
- New Delta Live Tables lessons and labs
	- Includes SQL and Python, not just SQL
	- Processing CDC data with APPLY CHANGES INTO, etc
- Removed multi-hop architecture and incremental processing with structured streaming
- Added Pyspark code for Spark SQL queries transformation lessons
- Added Python UDF lessons
- Condensed Delta Lake, Relational Entities, and ETL modules into Transform Data with Spark and Manage Data with Delta Lake

Cool stuff! Doesn't change anything I've taken notes on already

## Continuing DE 2.4 - Cleaning Data

### Validation work
#### 3:04
```sql

SELECT DISTINCT(*) FROM {table}
```
```python

df.distinct().count()
```
Both of the above cmd blocks do the same thing, all future cases where two are stacked like this means sql=python
Both of these remove entire rows where *all* the values match across all columns (completely duplicate rows, not a single duplicate value)

```sql
CREATE OR REPLACE TEMP VIEW deduped_users AS
SELECT user_id, user_timestamp, max(email) as email, max(updated) as updated
FROM users_dirty
WHERE user_id is not null
GROUP BY user_id, user_timestamp;

SELECT count(*) FROM deduped_users
```
```python
from pyspark.sql.functions import max
deduped_df = (
	users_df
	.where(col("user_id").isNotNull())
	.groupby("user_id", "user_timestamp")
	.agg(max("email").alias("email"),
		max("updated").alias("updated"))
)
deduped_df.count()
```
In both of the above code blocks, all cases where user_id and user_timestamp are duplicated, the entire row is removed even if the rest doesn't match (the way to delete duplicates based on x columns)
The max function keeps a non-null `email` and `updated` value. This is so if the first entry is kept but has nulls for these and one of the deleted rows has a value, we will still get a value instead of null
Fun lil hack

```sql
SELECT max(row_count) <= 1 no_duplicate_ids
FROM (
	SELECT user_id, count(*) as row_count
	FROM deduped_users
	GROUP BY user_id
)
```
```python
from pyspark.sql.functions import count

display(
		deduped_df
		.groupby("user_id")
		.agg(count("*").alias("row_count"))
		.select((max("row_count") <= 1).alias("no_duplicate_ids"))
)
```
These code blocks validate that `user_id` for each row is unique.
Just a programmatic way to verify that we don't have any duplicate columns instead of visual inspection

```sql
SELECT max(user_id_count) <= 1 at_most_one_id
FROM(
	SELECT email, count(user_id) as user_id_count
	from deduped_users
	where email is not null
	GROUP BY email
)
```
```python
display(deduped_df
	.where(col("email").isNotNull())
	.groupby("email")
	.agg(count("user_id").alias("user_id_count"))
	.select((max("user_id_count") <= 1).alias("at_most_one_id"))
)
```
Confirm that each `email` is associated with at most one `user_id`

### Date Format and Regex
#### 4:31

```sql
select *,
	date_format(first_touch, "MMM d, yyyy") as first_touch_date,
	date_format(first_touch, "HH:mm:ss") as first_touch_time,
	regexp_extract(email, "(?<=@).+", s) as email_domain
from(
	select *,
		cast(user_first_touch_timestamp / 1e6 as timestamp) as first_touch
	from deduped_users
)
```
```python
from pyspark.sql.functions import date_format, regexp_extract

display(deduped_df
	.withColumn("first_touch", (col("user_first_touch_timestamp") / 1e6).case("timestamp"))
	.withColumn("first_touch_date", date_format("first_touch", "MMM d, yyyy"))
	.withColumn("first_touch_time", date_format("first_touch", "HH:mm:ss"))
	.withColumn("email_domain", regexp_extract("email", "(?<=@).+"))	   
)
```
Scale and cast `user_first_touch_timestamp` to a valid timestamp
Extract calendar data and clock time for timestamp in human readable form
Use `regexp_extract` to extract just the domains from the email column

DE 2.4 is done

## Starting DE 2.5 Complex Transformation
Tabular data is fast and easy in databricks
Not all data is tabular though.

### Objectives
- Use `.` and `:` syntax on nested data
- Parse json strings into structs
- Flatten and unpack arrays and structs
- Combine datasets with joins
- Reshape data with pivot tables

### Data Overview
1:02

```sql
create or replace temp view events_strings as
select string(key), string(value)
from events_raw;

select * from events_strings
```
```python
from pyspark.sql.functions import col

events_strings_df = (spark
	.table("events_raw")
	.select(col("key").cast("string"),
		col("value").cast("string"))
)
```
Cast `key` and `value` into human readable format rather than the default binary-encoded json values

### Nested Data
1:23

```sql
select *
from events_strings
where value:event_name = "finalize"
order by key
limit 1
```
```python
display(events_strings_df
	.where("value:event_name = 'finalize'")
	.orderBy("key")
	.limit(1)
)
```
Query the converted string fields from above 
`:` is used to access subfields in JSON strings
`.` is used to access subfields in struct types

```sql
create or replace temp view parsed_events as select json.* from(
	select from_json(value, schema_of_json('{
		"device": "linux",
		"ecommerce": {
			"purchase_revenue_in_usd": 1875.5,
			"total_item_quantity": 1,
			"unique_items":1
		},
		"event_name": "finalize",
		...
	}')) as json
	from events_strings);

select * from parsed_events
)
```
```python
from pyspark.sql.functions import from_json, schema_of_json
json_string = """
{
	"device": "linux",
	"ecommerce": {
		"purchase_revenue_in_usd": 1875.5,
		"total_item_quantity": 1,
		"unique_items":1
	},
	"event_name": "finalize",
	...
}
"""
parsed_events_df = (events_strings_df
	.select(from_json("value", schema_of_json(json_string)).alias("json"))
	.select("json.*")
)
display(parsed_events_df)
```
Parse schema into struct types
`schema_of_json()` - Returns schema from a JSON string
`from_json()` - Parses a column containing a JSON string into a struct type using specified schema
`select json.*` - Flattens the struct, pulls out all subfields from json into their own columns

### Manipulate Arrays
2:50

```sql
create or replace temp view exploded_events as
select *, explode(items) as item
from parsed_events;

select * from exploded_events where size(items) > 2
```
```python
from pyspark.sql.functions import explode, size
exploded_events_df = (parsed_events_df
	.withColumns("item", explode("items"))
)

display(exploded_events_df.where(size("items") > 2))
```
`explode()` separates elements of an array into multiple new rows
`size()` counts the number of elements in an array for each row
Splits items (an array of structs) into multiple rows, only shows events with arrays with 3 or more items
explode([{"coupon": "null", "item_id": "ABCD"}, {"coupon": "1", "item_id": "ABCDE"}, {"coupon": "2", "item_id": "ABCDEF"}]) gives {"coupon": "null", "item_id": "ABCD"} on it's own row, {"coupon": "1", "item_id": "ABCDE"} on it's own row, etc. The size for each is 2

```sql
select
	user_id,
	collect_set(event_name) as event_history,
	array_distinct(flatten(collect_set(items.item_id))) as cart_history
from exploded_events
group by user_id
```
```python
from pyspark.sql.functions import array_distinct, collect_set, flatten

display(exploded_events_df
	.groupby("user_id")
	.agg(collect_set("event_name").alias("event_history"),
		array_distinct(flatten(collect_set("items.item_id"))).alias("cart_history")
	)
)
```
`collect_set()` collects unique values for a field, including fields in arrays
`flatten()` combines multiple arrays into a single array
`array_distinct()` removes duplicate elements from an array
Create table showing unique collection of actions and the items in a users cart

### Join Tables
3:54

```sql
create or replace temp view item_puyrchases as
select *
from (select *, explode(items) as item from sales) a
inner join item_lookup b
on a.item.item_id = b.item_id

select * from item_purchases
```
```python
exploded_sales_df = (spark
	.table("sales")
	.withColumn("item", explode("items"))
)

items_df = spark.table("item_lookup")

item_purchases_df = (exploded_sales_df
	.join(items_df, exploded_sales_df.item.item_id == items_df.item_id)
)

display(item_purchases_df)
```
Use `inner join`s to join exploded dataset and lookup table to get standard printed item name (name)

### Pivot Tables
4:27

```sql
select *
from item_purchases
pivot(
	sum(item.quantity) for item_id in (
		"p_foan_k",
		"m_stan_q",
		...
	)
)
```
```python
transactions_df = (item_purchases_df
	.groupBy(
		"order_id", # Resulting Columns here
		"email",
		"transaction_timestamp",
		"total_item_quantity",
		...
	)
	.pivot("item_id")
	.sum("item.quantity")
)
display(transactions_df)
```
`pivot` flattens out information from rows with nested data into new columns
Creates columns from nested data

Completed DE 2.5