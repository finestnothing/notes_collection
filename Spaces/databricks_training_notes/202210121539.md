source:: Data Engineering with Databricks - Module 1

# 202210121539
Created: 2022-10-12 15:39

## Databricks Architecture
- Databricks
	- Web application
	- Repos/notebooks
	- Job scheduling
	- Cluster Management (clusters run in the cloud account)
- Cloud Account (AWS)
	- Data processing plane (spark)
	- Data Sources
	- Databricks File System

## Clusters
- All purpose clusters
	- Interactively use notebooks
	- Use CLI or notebooks to create these clusters
	- Multiple users can share these
	- Retains up to 70 clusters for up to 30 days, can pin to save for longer
- Job Clusters
	- Created when a job is automated and run
	- Cannot start job clusters, autostart and autostop
	- Config info retains 30 clusters

## Create and Manage interactive clusters
- To Create a cluster:
	- Compute page
	- Create cluster button
	- Create cluster name
	- Select runtime
	- Select Node type
	- Add any other tags
	- Add logging, init scripts, ssh, docker, spark, etc (if relevant, probably not)

#to/research - Pools in databricks. Set the policy as spot with on demand fallback?

## Git Versioning
- Databricks repos built in to databricks
- Supports:
	- Azure devops
	- bitbucket
	- github
	- gitlab
- Can:
	- Clone repo
	- Pull changes (add any changes)
	- Pushing (sending changes)
	- Resetting
	- Branching
- CI/CD = Continuous Integration, Continuous Delivery
- CI/CD Workflows
	- Admin
		- Setup top-level repo folders (production, development, etc) (databricks)
		- Setup git automation to update repos on merge (git provider)
	- Workflow in databricks (done in databricks)
		- Clone remote repo to user folder
		- Create branch
		- Create and edit code
		- commit changes
		- push to branch
		- Go to Merge workflow
	- Merge workflow (done in git provider, Azure in our case)
		- pull request and review process
		- Merge into main
		- git automation calls databricks repos api
		- Production job workflow
	- Production job workflow (done in databricks)
		- API call brings repo in production folder to latest version
		- Run databricks job based on repo in production folder

## Using Databricks Repos
- To connect account to databricks, need personal access token (pat)
- Add personal access token to account
	- Settings > user settings > git integration > add username and personal access token
	- pat is stored in secrets
- To add repo to databricks
	- Go to repos
	- Click add repo
	- paste link to git repo
		- provider and repo name should auto populate
- Branching
	- Repos will go under "folders" from top layer
	- Next to each actual repo, the branch name is next to it
	- Click button with branch name
		- It will give details about the branch, and allow you to commit and push changes
	- Click "Create branch" on top left (next to branch name)
	- It will automatically switch the environment to new branches
	- The branch and commit page is also available within each file in a repo